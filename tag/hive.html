<!DOCTYPE html>
<html lang="zh_cn">
<head>
        <meta charset="utf-8" />
        <title>ytluck—For you - Hive</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">ytluck—For you </a></h1>
                <nav><ul>
                    <li><a href="http://ytluck.github.io/">Home</a></li>
                    <li><a href="/About me.html">About Me</a></li>
                    <li><a href="/big-data/index.html">Big Data</a></li>
                    <li><a href="/data-mining/index.html">Data Mining</a></li>
                    <li><a href="/items/index.html">Items</a></li>
                    <li><a href="/os/index.html">OS</a></li>
                    <li><a href="/program/index.html">Program</a></li>
                </ul></nav>
        </header><!-- /#banner -->

            <aside id="featured" class="body">
                <article>
                    <h1 class="entry-title"><a href="/data-mining/my-dataming-post-42.html">Hive的执行和优化</a></h1>
<footer class="post-info">
        <abbr class="published" title="2019-03-24T20:52:00+08:00">
                Published: 2019-03-24 20:52:00
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ytwan.html">ytwan</a>
        </address>
<p>In <a href="/data-mining/index.html">Data Mining</a>.</p>
<p>tags: <a href="/tag/hive.html">Hive</a> </p>
</footer><!-- /.post-info --><h3>Hive的调优</h3>
<div class="highlight"><pre><span></span>选取执行引擎： mr tez spark
        set hive execution.engine=spark
选取文件存储的格式：
优化器的选择：set  hive.cbo.enable=true
表统计信息的收集： set  hive.stats.autogather=true  &lt;默认开启&gt;
</pre></div>


<h3>数据倾斜：Hive on MapReduce</h3>
<div class="highlight"><pre><span></span>-- Mapreduce的优化核心在确保每个节点数据分布均匀
系统层面：
    1.Job数
    2. map reduce join groupby的设置
Hive数据倾斜原因
       key分布不均匀
       业务数据本身的特性
       SQL语句造成数据倾斜
代码层面：
    尽量尽早地过滤数据 -- 过滤无效ID以及设置null值
    不同数据类型关联前的转换
    列裁剪和分区裁剪
    将大表放后头
    Limit语句还是需要执行整个查询语句，然后再返回部分结果。
      有一个配置属性可以开启，避免这种情况---对数据源进行抽样
      hive.limit.optimize.enable=true --- 开启对数据源进行采样的功能
      hive.limit.row.max.size --- 设置最小的采样容量
      hive.limit.optimize.limit.file --- 设置最大的采样样本数
</pre></div>


<h3>优化方式：</h3>
<div class="highlight"><pre><span></span>针对单个Job 中单个步骤的优化，针对MR全局的优化
</pre></div>


<h4>1.map阶段的优化：</h4>
<div class="highlight"><pre><span></span>通过Input和blocksize来决定MR任务的数量
01.产生的主要原因：上游表文件大小不均匀，且小文件居多
   解决方案：
       对上游合并小文件，同时调整map 个数
    具体设置参数：
        上游合并小文件
                合并小文件
                    hive.merg.mapfiles=true：合并map输出 
                    hive.merge.mapredfiles=false：合并reduce输出 
                    hive.merge.size.per.task=256*1000*1000：合并文件的大小 
                    hive.mergejob.maponly=true：如果支持CombineHiveInputFormat则生成只有Map的任务执行merge 
                    hive.merge.smallfiles.avgsize=16000000：文件的平均大小小于该值时，会启动一个MR任务执行merge。
        调整map数
            map读取文件的大小： set  dfs.block.size
            map 的个数   set mapred.min.split.size
                    set mapred.max.split.size
    02产生原因：map端聚合操作-某个值特别多
        解决方案：
            1.代码中 distributed by rand() 将map端分发重新按照随机值进行分发，这样map端只负责数据分发了           
            2.hive.map.aggr=false  关闭聚合操作放在Map阶段执行，不建议
             map端聚合-- 指在group by语句之前执行的聚合操作
</pre></div>


<h4>2.reduce端的优化</h4>
<div class="highlight"><pre><span></span> reduce端--主要对map端梳理后的key-value键值对进行聚合
    set hive.exec.reducers.bytes.per.reducer
    set hive.exec.reducer.max
    reduce端可以直接设置任务数，如果设置了，则直接使用
        set hive.exce.reduce.tasks
</pre></div>


<h4>3.Join操作倾斜：</h4>
<div class="highlight"><pre><span></span> Join操作需要参与map和reduce的整个阶段
分为
01.每路数据值都要，且空值多-导致 (空值连接)- 将空值过滤掉或者给随机数区间
02.大小数据--启动mapjoin  -会把小表自动加载到内存中，箱单与mapjoin ，没有shuffle阶段
    set hive.auto.conver.join= true
    set hive.mapjoin.smalltable.filesize
03.Join中有热点数据导致
    对热点和非热点数据分别处理，然后再合并union
其余： 设置
    set hive.optimize.skewjoin=true
</pre></div>


<h4>4.Group造成的倾斜</h4>
<div class="highlight"><pre><span></span>    hive.map.aggr=true 在map端做部分聚合
默认该参数的值为false，表示不启用，要启用时，可以set hive.groupby.skewindata=ture;进行启用
</pre></div>


<h3>针对多个Job - job的并行</h3>
<div class="highlight"><pre><span></span>其中 set hive.exec.parallel=true,可以开启并发执行。可以控制一个sql中多个可并行执行的job的运行方式，提高job的并发
其中 hive.exec.parallel.thread.number 表示同时运行的job的个数--减少job数
</pre></div>


<h3>Hive执行过程--驱动器：Driver</h3>
<div class="highlight"><pre><span></span>调用Driver中的run方法开始执行编译，解析，分析(通过元数据)生成hive.jar交给hadoop执行，添加task线程启动线程，
   最后通过Runtime类在jvm中执行并返回结果。
1.语法解析器  将SQL抽象为 语法树                解析器（SQL Parser）
2.语义分析    将AST抽象为  单元查询块        编译器（Physical Plan）
3.将单元查询快转换为操作树                      编译器（Physical Plan）
4.逻辑优化器对操作树进行优化                 优化器（Query Optimizer）
5.将优化的操作树翻译为Mapreduce任务
6.Hive物理优化器-优化MR任务--生成执行计划
7.执行MR任务
</pre></div>


<h3>Hive中的操作</h3>
<div class="highlight"><pre><span></span>查看执行计划
    explain 结果会有三部分
         AST
         the Dependencies Graph
         STAGEthe plans of each stage
基于代价的优化器  CBO
基于rule的优化器    CRO
一个hive任务分为一个或多个stage阶段。
不同的stage阶段会存在依赖关系，越复杂的sql会产生越多的stage,通常也需要越多的时间
</pre></div>


<h4>MR- MapReduce：</h4>
<div class="highlight"><pre><span></span>MR有两个阶段组成：Map和Reduce，用户只需实现map()和reduce()两个函数，即可实现分布式计算
shuffle：
    多个map任务的输出，按照不同的分区，通过网络copy到不同的reduce节点上。
    Spark较之MR快在哪里
Spark对MapReduce进行优化
 01.减少磁盘ID
    Map端将中间数据输出和结果存储在磁盘中
    reduce端要从磁盘读写中间结构
 02.增加并行度
    Spark把不同的环节抽象为stage，允许多个stage既可以串行也可以并行
 03.可选的Shuffle排序
     MapReduce在shuffle之前有固定的排序，Spark可以选择在Map端还是reduce端排序
 04.Spark的内存管理策略
</pre></div>


<h3>.Hive开发UDF</h3>
<div class="highlight"><pre><span></span>  UDF   UDAF (用户自定义聚合函数) ， UDTF ( 用户自定义表生成函数 )
    UDF(user-defined functions) 
    UDAF(user-defined aggregation functions) 
    UDTF(user-defined table-generating functions)
</pre></div>


<h3>Hive的配置</h3>
<div class="highlight"><pre><span></span>Hive的配置管理参数时，
    参数分为四类：
        Hive管理参数、
        Hive元存储（Metastore）管理参数、
        与Hadoop交互的管理参数、
        用于传递运行时信息的参数
    01.Hive管理参数
         hive.execution.engin           hive的计算引擎   
         控制hive任务的reduce数、内存
    02.Hive元存储（Metastore）管理参数：
        MetaStore：metastore是个独立的关系数据库，用来持久化schema和系统元数据--元存储服务器
        meta数据存储方式：元存储服务器
            本地derby-- 内嵌式元存储
            本地mysql
            远端mysql--存储方式需要在远端服务器运行一个mysql服务器，并且需要在Hive服务器启动meta服务
                        元数据服务器和客户端之间使用Thrift协议通信
        客户端的配置为：
                   hive.metastore.uris                  Thrift元存储服务器的主机和端口号
               hive.metastore.warehouse.dir    数据仓库的位置，默认是/user/hive/warehouse；
     其他：UDF的使用
</pre></div>                </article>
            </aside><!-- /#featured -->
                <section id="content" class="body">
                    <h1>Other articles</h1>
                    <hr />
                    <ol id="posts-list" class="hfeed">

            <li><article class="hentry">
                <header>
                    <h1><a href="/big-data/my-bigdata-post-28.html" rel="bookmark"
                           title="Permalink to 阿里云离线计算初步使用">阿里云离线计算初步使用</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2017-12-16T19:02:00+08:00">
                Published: 2017-12-16 19:02:00
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ytwan.html">ytwan</a>
        </address>
<p>In <a href="/big-data/index.html">Big Data</a>.</p>
<p>tags: <a href="/tag/hive.html">Hive</a> </p>
</footer><!-- /.post-info -->                <p>数据仓库-Maxcompute-离线计算</p>
                <a class="readmore" href="/big-data/my-bigdata-post-28.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>

            <li><article class="hentry">
                <header>
                    <h1><a href="/big-data/my-bigdata-post-26.html" rel="bookmark"
                           title="Permalink to Hive高阶数据查询">Hive高阶数据查询</a></h1>
                </header>

                <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2017-08-06T19:02:00+08:00">
                Published: 2017-08-06 19:02:00
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ytwan.html">ytwan</a>
        </address>
<p>In <a href="/big-data/index.html">Big Data</a>.</p>
<p>tags: <a href="/tag/hive.html">Hive</a> </p>
</footer><!-- /.post-info -->                <p>Hive内置运算和内置函数及常用场景</p>
                <a class="readmore" href="/big-data/my-bigdata-post-26.html">read more</a>
                </div><!-- /.entry-content -->
            </article></li>
                </ol><!-- /#posts-list -->
                </section><!-- /#content -->
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="https://github.com/ytluck">yt</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="http://pelican-zh.readthedocs.org/en/latest/zh-cn/settings/">ye</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-76643035-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = 'https://ssl.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>
</body>
</html>