<!DOCTYPE html>
<html lang="zh_cn">
<head>
        <meta charset="utf-8" />
        <title>HDFS和Spark的交互</title>
        <link rel="stylesheet" href="/theme/css/main.css" />

        <!--[if IE]>
            <script src="https://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">ytluck—For you </a></h1>
                <nav><ul>
                    <li><a href="http://ytluck.github.io/">Home</a></li>
                    <li><a href="/About me.html">About Me</a></li>
                    <li class="active"><a href="/big-data/index.html">Big Data</a></li>
                    <li><a href="/data-mining/index.html">Data Mining</a></li>
                    <li><a href="/items/index.html">Items</a></li>
                    <li><a href="/os/index.html">OS</a></li>
                    <li><a href="/program/index.html">Program</a></li>
                </ul></nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="/big-data/my-bigdata-post-17.html" rel="bookmark"
           title="Permalink to HDFS和Spark的交互">HDFS和Spark的交互</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <abbr class="published" title="2017-01-08T09:02:00+08:00">
                Published: 2017-01-08 09:02:00
        </abbr>

        <address class="vcard author">
                By                         <a class="url fn" href="/author/ytwan.html">ytwan</a>
        </address>
<p>In <a href="/big-data/index.html">Big Data</a>.</p>
<p>tags: <a href="/tag/things.html">things</a> </p>
</footer><!-- /.post-info -->      <h3>Spark使用Scala连接HDFS</h3>
<div class="highlight"><pre><span></span>总共分为三个步骤
第一步：配置包依赖Maven
第二步：配置文件的参数的设置与读写.properties 在HDFS暂时没有使用
第三步：编写程序

    //第一步：创建SparkConf对象配置一些通用的属性，并且基于这个创建一个SparkContext对象setMaster(&quot;local[2]&quot;)
    val SparkConf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;CountTest&quot;).set(&quot;spark.executor.memory&quot;, &quot;1g&quot;)
    //使用SparkContext对象中的textFile函数将输入文件转换为一个RDD ，SparkContext对象代表对计算机集群的一个连接
    val sc = new SparkContext(SparkConf)
    //路径前面加上file:// 表示从本地文件系统读；读取本地文本文件，创建一个字符串的RDD
    val fluxRDD = sc.textFile(&quot;file:///C:\\Items.json&quot;)
    //在路径前面加上hdfs://表示从hdfs文件系统；读取HDFS中文件参数是一个path,这个path
    // val fluxRDD = sc.textFile(&quot;hdfs://localhost:8102/15*.json&quot;)
    //hdfs://master:port/path
</pre></div>


<h3>Spark存储对象到HDFS</h3>
<h4>保存RDD为对象</h4>
<div class="highlight"><pre><span></span>    将RDD保存为对象saveAsObjectFile()
   .objectFile()函数接收路径，返回对应的RDD
</pre></div>


<h4>保存模型到RDD上为对象</h4>
<div class="highlight"><pre><span></span><span class="o">//</span><span class="err">将上面的模型存储到</span><span class="n">hdfs</span><span class="o">---</span><span class="n">Spark</span><span class="err">环境下生成一个</span><span class="n">word2vec</span><span class="err">模型</span><span class="p">,</span> <span class="err">存储到</span><span class="n">hdfs</span><span class="o">.</span>
<span class="n">ObjectInputStream</span><span class="err">与</span><span class="n">ObjectOutputStream</span><span class="err">类所读写的对象必须实现</span><span class="n">Serializable</span><span class="err">接口，只能将支持</span> <span class="n">java</span><span class="o">.</span><span class="n">io</span><span class="o">.</span><span class="n">Serializable</span> <span class="err">接口的对象写入流中。对象中的</span><span class="n">transient</span><span class="err">和</span><span class="n">static</span><span class="err">类型成员变量不会被读取和写入</span>
<span class="err">可以使用</span><span class="n">ObjectOutputStream</span> <span class="err">将</span> <span class="n">Java</span> <span class="err">对象的基本数据类型和图形写入</span> <span class="n">OutputStream</span><span class="err">。</span>
<span class="err">可以使用</span> <span class="n">ObjectInputStream</span> <span class="err">读取（重构）对象。通过在流中使用文件可以实现对象的持久存储</span>
<span class="n">val</span> <span class="n">hadoopConf</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">hadoopConfiguration</span>
<span class="n">hadoopConf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;fs.defaultFS&quot;</span><span class="p">,</span> <span class="s2">&quot;hdfs://myhadoop:9000/&quot;</span><span class="p">)</span>
<span class="n">val</span> <span class="n">fileSystem</span> <span class="o">=</span> <span class="n">FileSystem</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">hadoopConf</span><span class="p">)</span>
<span class="n">val</span> <span class="n">path</span> <span class="o">=</span> <span class="n">new</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;/user/hadoop/data/mllib/word2vec-object&quot;</span><span class="p">)</span>
<span class="n">val</span> <span class="n">oos</span> <span class="o">=</span> <span class="n">new</span> <span class="n">ObjectOutputStream</span><span class="p">(</span><span class="n">new</span> <span class="n">FSDataOutputStream</span><span class="p">(</span><span class="n">fileSystem</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">path</span><span class="p">)))</span>
<span class="n">oos</span><span class="o">.</span><span class="n">writeObject</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">oos</span><span class="o">.</span><span class="n">close</span>
<span class="p">}</span>

<span class="o">//</span><span class="err">这里示例另外一个程序直接从</span><span class="n">hdfs</span><span class="err">读取序列化对象使用模型</span>
<span class="n">val</span> <span class="n">ois</span> <span class="o">=</span> <span class="n">new</span> <span class="n">ObjectInputStream</span><span class="p">(</span><span class="n">new</span> <span class="n">FSDataInputStream</span><span class="p">(</span><span class="n">fileSystem</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">path</span><span class="p">)))</span>
<span class="n">val</span> <span class="n">sample_model</span> <span class="o">=</span> <span class="n">ois</span><span class="o">.</span><span class="n">readObject</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="p">[</span><span class="n">Word2VecModel</span><span class="p">]</span>

  <span class="o">/*</span>
    <span class="o">*</span> <span class="o">//</span><span class="err">以将序列化文件从</span><span class="n">hdfs</span><span class="err">放到本地</span><span class="p">,</span> <span class="n">scala</span><span class="err">程序使用模型</span>
    <span class="o">*</span> <span class="kn">import</span> <span class="nn">java.io._</span>
    <span class="o">*</span> <span class="kn">import</span> <span class="nn">org.apache.spark.mllib.feature.</span><span class="p">{</span><span class="n">Word2Vec</span><span class="p">,</span> <span class="n">Word2VecModel</span><span class="p">}</span>
    <span class="o">*</span> <span class="n">val</span> <span class="n">ois</span> <span class="o">=</span> <span class="n">new</span> <span class="n">ObjectInputStream</span><span class="p">(</span><span class="n">new</span> <span class="n">FileInputStream</span><span class="p">(</span><span class="s2">&quot;/home/cherokee/tmp/word2vec-object&quot;</span><span class="p">))</span>
    <span class="o">*</span> <span class="n">val</span> <span class="n">sample_model</span> <span class="o">=</span> <span class="n">ois</span><span class="o">.</span><span class="n">readObject</span><span class="o">.</span><span class="n">asInstanceOf</span><span class="p">[</span><span class="n">Word2VecModel</span><span class="p">]</span>
    <span class="o">*</span> <span class="n">ois</span><span class="o">.</span><span class="n">close</span>
    <span class="o">*/</span>
</pre></div>


<h3>Hadoop现在同时提供了新旧的两套API接口</h3>
<div class="highlight"><pre><span></span>旧版API 放在org.apache.hadoop.mapred 包中，
而新版API 则放在org.apache.hadoop.mapreduce 包及其子包中
</pre></div>


<h4>老式</h4>
<div class="highlight"><pre><span></span>hadoopFile
saveAsHadoopFile()
</pre></div>


<h4>新式</h4>
<div class="highlight"><pre><span></span> hadoopDataset    saveAsHadoopDataset     
 newAPIHadoopDataset saveAsNewAPIHadoopDataset
                    saveAsNewAPIHadoopFile

val HBase_DATARDD = sc.newAPIHadoopRDD(hbaseConf, classOf[TableInputFormat], classOf[ImmutableBytesWritable], classOf[Result])
    //脚手架语句，查看结果
</pre></div>


<h3>参考</h3>
<div class="highlight"><pre><span></span>Spark中将对象序列化存储到hdfs https://my.oschina.net/waterbear/blog/525347
</pre></div>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="https://github.com/ytluck">yt</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="http://pelican-zh.readthedocs.org/en/latest/zh-cn/settings/">ye</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>, which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->

                <p>The theme is by <a href="http://coding.smashingmagazine.com/2009/08/04/designing-a-html-5-layout-from-scratch/">Smashing Magazine</a>, thanks!</p>
        </footer><!-- /#contentinfo -->

    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-76643035-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = 'https://ssl.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
    </script>
</body>
</html>